{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pegasus_paraphrase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1cHu7nBKKjQTqvQIRyfsYXkPC4y0veyF5",
      "authorship_tag": "ABX9TyOrN8hUZOJ8Glf28ZwZN6Dy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Augment4Gains/blob/main/pegasus_paraphrase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKDejEfrrkZh"
      },
      "source": [
        "# Using a pre-trained Transformer Encoder-Decoder based Paraphraser"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oJZEy3KtphC"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Getting necessary libraries\n",
        "!pip install -U transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import nltk\n",
        "import sentencepiece\n",
        "from tqdm import tqdm\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BertTokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7IF3tqit3Uv",
        "outputId": "a21cf1ea-cd1e-488c-cfc8-e4112991400b"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EpUtRKc35Bf"
      },
      "source": [
        "# Using a standard model\n",
        "tokenizer_model_name = 'google/pegasus-large'\n",
        "\n",
        "# Using a pre-trained community model\n",
        "paraphrasing_model_name = 'tuner007/pegasus_paraphrase'\n",
        "\n",
        "torch_device = 'cuda'\n",
        "if torch.cuda.is_available() == False:\n",
        "  torch_device = 'cpu'\n",
        "\n",
        "tokenizer = PegasusTokenizer.from_pretrained(tokenizer_model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(paraphrasing_model_name).to(torch_device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2FxfvLyK63A"
      },
      "source": [
        "def get_paraphrase(input_text, num_return_sequences = 2, num_beams = 10):\n",
        "  preprocess = True\n",
        "  preprocess_len = 52\n",
        "  max_len = 60\n",
        "\n",
        "  if preprocess == True:\n",
        "    token_count = 0\n",
        "    processed_text = ''\n",
        "    \n",
        "    sentences = nltk.sent_tokenize(input_text)\n",
        "    for sentence in sentences:\n",
        "      tokens = nltk.word_tokenize(sentence)\n",
        "      count = len(tokens)\n",
        "      token_count += count\n",
        "      if token_count > preprocess_len:\n",
        "        break\n",
        "      for token in tokens:\n",
        "        processed_text += (token + ' ')\n",
        "  else:\n",
        "    processed_text = input_text\n",
        "\n",
        "  batch = tokenizer([processed_text], \n",
        "                    truncation = True,\n",
        "                    padding = 'longest',\n",
        "                    max_length = max_len,\n",
        "                    return_tensors = 'pt').to(torch_device)\n",
        "\n",
        "  translated = model.generate(**batch,\n",
        "                              max_length = max_len,\n",
        "                              num_beams = num_beams,\n",
        "                              num_return_sequences = num_return_sequences,\n",
        "                              temperature = 1.5)\n",
        "  \n",
        "  target = tokenizer.batch_decode(translated,\n",
        "                                  skip_special_tokens = True)\n",
        "  \n",
        "  return target"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt8CTX4avo4h",
        "outputId": "6c22fd7e-12fe-44e8-affe-5d3ac2a93ca3"
      },
      "source": [
        "# Viewing Sample Paraphrases\n",
        "\n",
        "examples = ['you should watch louis le vau \\'s latest video . steven oh of tyt is disturbing as hell and makes me hope that jimmy dore wakes the left up .',\n",
        "            'kill yourself you whiny , self-righteous faggot .',\n",
        "            'but why do they make that face']\n",
        "\n",
        "for example in examples:\n",
        "  print('Source: ' + str(example))\n",
        "  responses = get_paraphrase(example)\n",
        "  print('Primary Paraphrase: ' + str(responses[0]))\n",
        "  print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: you should watch louis le vau 's latest video . steven oh of tyt is disturbing as hell and makes me hope that jimmy dore wakes the left up .\n",
            "Primary Paraphrase: louis le vau's latest video is disturbing and makes me hope that jimmy dore wakes the left up.\n",
            "\n",
            "Source: kill yourself you whiny , self-righteous faggot .\n",
            "Primary Paraphrase: You are self-righteous and should kill yourself.\n",
            "\n",
            "Source: but why do they make that face\n",
            "Primary Paraphrase: Why do they make that face?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cx3G5pYvu31"
      },
      "source": [
        "# Generating the Augmented Training Data\n",
        "set_type = 'reddit'\n",
        "\n",
        "# Reference to the absolute path in Google Drive\n",
        "data_path = 'drive/My Drive/Augment4Gains/data/' + set_type\n",
        "\n",
        "with open(data_path + '/' + 'train.json', 'r+') as f:\n",
        "  raw_train = json.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj_zq6f0FvLA",
        "outputId": "54c7a06d-c519-49b3-b9dd-70ccb39d1f1f"
      },
      "source": [
        "# Getting the Augmented Datapoints\n",
        "augmented_data = []\n",
        "limit = len(raw_train)\n",
        "minimum_length = 4\n",
        "\n",
        "test_mode = True\n",
        "if test_mode == True:\n",
        "  limit = 20\n",
        "\n",
        "for index, unit in enumerate(tqdm(raw_train, total = limit)):\n",
        "  if index > limit - 1:\n",
        "    break\n",
        "\n",
        "  try:\n",
        "    raw_text = str(unit['source'].replace('\\n', ' '))\n",
        "    target = get_paraphrase(raw_text)[0]\n",
        "  except:\n",
        "    pass\n",
        "    continue\n",
        "  \n",
        "  token_count = len(nltk.word_tokenize(target))\n",
        "  if token_count < minimum_length:\n",
        "    continue\n",
        "  new_unit = unit.copy()\n",
        "  \n",
        "  if 'type' in new_unit:\n",
        "    new_unit.pop('type')\n",
        "  if 'set' in new_unit:\n",
        "    new_unit.pop('set')\n",
        "\n",
        "  new_unit['source'] = target\n",
        "  augmented_data.append(new_unit)\n",
        "\n",
        "overwrite_data = True\n",
        "\n",
        "if overwrite_data == True:\n",
        "  with open(data_path + '/' + 'paraphrased_train.json', 'w+') as f:\n",
        "    json.dump(augmented_data, f)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:09<00:00,  2.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkswWKTF040"
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}