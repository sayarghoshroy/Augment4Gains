@inproceedings{zampieri-etal-2019-semeval,
    title = "{S}em{E}val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media ({O}ffens{E}val)",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Farra, Noura  and
      Kumar, Ritesh",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S19-2010",
    doi = "10.18653/v1/S19-2010",
    pages = "75--86",
}

@article{arora2016simple,
  title={A simple but tough-to-beat baseline for sentence embeddings},
  author={Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  year={2016}
}

@proceedings{ws-2018-trolling,
    title = "Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",
    editor = "Kumar, Ritesh  and
      Ojha, Atul Kr.  and
      Zampieri, Marcos  and
      Malmasi, Shervin",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-4400",
}

@inproceedings{germeval-task-2,
author = {Struß, Julia and Siegel, Melanie and Ruppenhofer, Josef and Wiegand, Michael and Klenner, Manfred},
year = {2019},
month = {10},
pages = {},
title = {Overview of GermEval Task 2, 2019 Shared Task on the Identification of Offensive Language}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{10.1145/2872427.2883062,
author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
title = {Abusive Language Detection in Online User Content},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883062},
doi = {10.1145/2872427.2883062},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {145–153},
numpages = {9},
keywords = {abusive language, stylistic classification, hate speech, discourse classification, nlp, natural language processing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@INPROCEEDINGS{6406271,  author={Y. {Chen} and Y. {Zhou} and S. {Zhu} and H. {Xu}},  booktitle={2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing},   title={Detecting Offensive Language in Social Media to Protect Adolescent Online Safety},   year={2012},  volume={},  number={},  pages={71-80},}

@inproceedings{character-abuse,
author = {Mehdad, Yashar and Tetreault, Joel},
year = {2016},
month = {01},
pages = {299-303},
title = {Do Characters Abuse More Than Words?},
doi = {10.18653/v1/W16-3638}
}

@inproceedings{chen2012detecting,
  title={Detecting offensive language in social media to protect adolescent online safety},
  author={Chen, Ying and Zhou, Yilu and Zhu, Sencun and Xu, Heng},
  booktitle={2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing},
  pages={71--80},
  year={2012},
  organization={IEEE}
}

@article{gitari2015lexicon,
  title={A lexicon-based approach for hate speech detection},
  author={Gitari, Njagi Dennis and Zuping, Zhang and Damien, Hanyurwimfura and Long, Jun},
  journal={International Journal of Multimedia and Ubiquitous Engineering},
  volume={10},
  number={4},
  pages={215--230},
  year={2015}
}

@misc{distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Kwok2013LocateTH,
  title={Locate the Hate: Detecting Tweets against Blacks},
  author={I. Kwok and Y. Wang},
  booktitle={AAAI},
  year={2013}
}
@inproceedings{wang2014cursing,
  title={Cursing in english on twitter},
  author={Wang, Wenbo and Chen, Lu and Thirunarayan, Krishnaprasad and Sheth, Amit P},
  booktitle={Proceedings of the 17th ACM conference on Computer supported cooperative work \& social computing},
  pages={415--425},
  year={2014}
}
@phdthesis{phdthesis,
author = {Themeli, Sissy},
year = {2018},
month = {10},
pages = {},
title = {Hate Speech Detection using different text representations in online user comments},
doi = {10.13140/RG.2.2.12991.25764}
}


@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}
@inproceedings{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in neural information processing systems},
  pages={649--657},
  year={2015}
}
@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@article{DBLP:journals/corr/EisnerRABR16,
  author    = {Ben Eisner and
               Tim Rockt{\"{a}}schel and
               Isabelle Augenstein and
               Matko Bosnjak and
               Sebastian Riedel},
  title     = {emoji2vec: Learning Emoji Representations from their Description},
  journal   = {CoRR},
  volume    = {abs/1609.08359},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08359}
 }
  
  @misc{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{badjatiya2017dlhate,
author = {Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
title = {Deep Learning for Hate Speech Detection in Tweets},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054223},
doi = {10.1145/3041021.3054223},
abstract = {Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {759–760},
numpages = {2},
keywords = {hate speech detection, cnn, lstm, deep learning applications, twitter},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{badjatiya2019persp,
author = {Badjatiya, Pinkesh and Gupta, Manish and Varma, Vasudeva},
title = {Stereotypical Bias Removal for Hate Speech Detection Task Using Knowledge-Based Generalizations},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313504},
doi = {10.1145/3308558.3313504},
abstract = {With the ever-increasing cases of hate spread on social media platforms, it is critical to design abuse detection mechanisms to pro-actively avoid and control such incidents. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. Bias removal has been traditionally studied for structured datasets, but we aim at bias mitigation from unstructured text data. In this paper, we make two important contributions. First, we systematically design methods to quantify the bias for any model and propose algorithms for identifying the set of words which the model stereotypes. Second, we propose novel methods leveraging knowledge-based generalizations for bias-free learning. Knowledge-based generalization provides an effective way to encode knowledge because the abstraction they provide not only generalizes content but also facilitates retraction of information from the hate speech detection classifier, thereby reducing the imbalance. We experiment with multiple knowledge generalization policies and analyze their effect on general performance and in mitigating bias. Our experiments with two real-world datasets, a Wikipedia Talk Pages dataset (WikiDetox) of size ~ 96k and a Twitter dataset of size ~ 24k, show that the use of knowledge-based generalizations results in better performance by forcing the classifier to learn from generalized content. Our methods utilize existing knowledge-bases and can easily be extended to other tasks. },
booktitle = {The World Wide Web Conference},
pages = {49–59},
numpages = {11},
keywords = {bias removal, stereotypical bias, bias detection, natural language processing, hate speech, knowledge-based generalization},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{reimers-2020-multilingual-sentence-bert,
    title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
    author = "Reimers, Nils and Gurevych, Iryna",
    journal= "arXiv preprint arXiv:2004.09813",
    month = "04",
    year = "2020",
    url = "http://arxiv.org/abs/2004.09813",
}

@inproceedings{hasoc2020overview,
title={{Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive Content Identification in Indo-European Languages)}},
author={Mandl, Thomas and Modha, Sandip and Shahi, Gautam Kishore and Jaiswal, Amit Kumar and Nandini, Durgesh and Patel, Daksh and Majumder, Prasenjit and Schäfer, Johannes},
booktitle={Working Notes of FIRE 2020 - Forum for Information Retrieval Evaluation},
publisher = {CEUR},
year={2020},
Month={December}
}

@article{mathew2020hatexplain,
  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  journal={arXiv preprint arXiv:2012.10289},
  year={2020}
}

@inproceedings{hurtlex,
  title={Hurtlex: A Multilingual Lexicon of Words to Hurt},
  author={E. Bassignana and Valerio Basile and V. Patti},
  booktitle={CLiC-it},
  year={2018}
}

@inproceedings{hurtbert,
    title = "{H}urt{BERT}: Incorporating Lexical Features with {BERT} for the Detection of Abusive Language",
    author = "Koufakou, Anna  and
      Pamungkas, Endang Wahyu  and
      Basile, Valerio  and
      Patti, Viviana",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.alw-1.5",
    doi = "10.18653/v1/2020.alw-1.5",
    pages = "34--43"
}



@article{xlmr,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  archivePrefix = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gaydhani2018detecting,
      title={Detecting Hate Speech and Offensive Language on Twitter using Machine Learning: An N-gram and TFIDF based Approach}, 
      author={Aditya Gaydhani and Vikrant Doma and Shrikant Kendre and Laxmi Bhagwat},
      year={2018},
      eprint={1809.08651},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{xlm,
  author    = {Guillaume Lample and
               Alexis Conneau},
  title     = {Cross-lingual Language Model Pretraining},
  journal   = {CoRR},
  volume    = {abs/1901.07291},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.07291},
  archivePrefix = {arXiv},
  eprint    = {1901.07291},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-07291.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{tapt,
      title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks}, 
      author={Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
      year={2020},
      eprint={2004.10964},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tapt-tathagata,
      title={Task Adaptive Pretraining of Transformers for Hostility Detection}, 
      author={Tathagata Raha and Sayar Ghosh Roy and Ujwal Narayan and Zubair Abid and Vasudeva Varma},
      year={2021},
      eprint={2101.03382},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hasoc-sayar,
      title={Leveraging Multilingual Transformers for Hate Speech Detection}, 
      author={Ghosh Roy, Sayar and Narayan, Ujwal and Raha, Tathagata and Abid, Zubair and Varma, Vasudeva},
      year={2021},
      eprint={2101.03207},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hate-data,
   title={Directions in abusive language training data, a systematic review: Garbage in, garbage out},
   volume={15},
   ISSN={1932-6203},
   url={http://dx.doi.org/10.1371/journal.pone.0243300},
   DOI={10.1371/journal.pone.0243300},
   number={12},
   journal={PLOS ONE},
   publisher={Public Library of Science (PLoS)},
   author={Vidgen, Bertie and Derczynski, Leon},
   editor={Grabar, NataliaEditor},
   year={2020},
   month={Dec},
   pages={e0243300}
}

@misc{benchmark,
      title={A Benchmark Dataset for Learning to Intervene in Online Hate Speech}, 
      author={Jing Qian and Anna Bethke and Yinyin Liu and Elizabeth Belding and William Yang Wang},
      year={2019},
      eprint={1909.04251},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{davidson2017automated,
      title={Automated Hate Speech Detection and the Problem of Offensive Language}, 
      author={Thomas Davidson and Dana Warmsley and Michael Macy and Ingmar Weber},
      year={2017},
      eprint={1703.04009},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{cao-lee-2020-hategan,
    title = "{H}ate{GAN}: Adversarial Generative-Based Data Augmentation for Hate Speech Detection",
    author = "Cao, Rui  and
      Lee, Roy Ka-Wei",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.557",
    doi = "10.18653/v1/2020.coling-main.557",
    pages = "6327--6338",
    abstract = "Academia and industry have developed machine learning and natural language processing models to detect online hate speech automatically. However, most of these existing methods adopt a supervised approach that heavily depends on labeled datasets for training. This results in the methods{'} poor detection performance of the hate speech class as the training datasets are highly imbalanced. In this paper, we propose HateGAN, a deep generative reinforcement learning model, which addresses the challenge of imbalance class by augmenting the dataset with hateful tweets. We conduct extensive experiments to augment two commonly-used hate speech detection datasets with the HateGAN generated tweets. Our experiment results show that HateGAN improves the detection performance of the hate speech class regardless of the classifiers and datasets used in the detection task. Specifically, we observe an average 5{\%} improvement for the hate class F1 scores across all state-of-the-art hate speech classifiers. We also conduct case studies to empirically examine the HateGAN generated hate speeches and show that the generated tweets are diverse, coherent, and relevant to hate speech detection.",
}

@inproceedings{AlexNet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {1097--1105},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{jha2020does,
      title={Does Data Augmentation Improve Generalization in NLP?}, 
      author={Rohan Jha and Charles Lovering and Ellie Pavlick},
      year={2020},
      eprint={2004.15012},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{liu-etal-2020-data,
    title = "Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation",
    author = "Liu, Ruibo  and
      Xu, Guangxuan  and
      Jia, Chenyan  and
      Ma, Weicheng  and
      Wang, Lili  and
      Vosoughi, Soroush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.726",
    doi = "10.18653/v1/2020.emnlp-main.726",
    pages = "9031--9041",
    abstract = "Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7{\%} on average when given only 10{\%} of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.",
}

@inproceedings{wei-zou-2019-eda,
    title = "{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
    author = "Wei, Jason  and
      Zou, Kai",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1670",
    doi = "10.18653/v1/D19-1670",
    pages = "6382--6388",
    abstract = "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50{\%} of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.",
}

@inproceedings{wang-yang-2015-thats,
    title = "That{'}s So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using {\#}petpeeve Tweets",
    author = "Wang, William Yang  and
      Yang, Diyi",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1306",
    doi = "10.18653/v1/D15-1306",
    pages = "2557--2563",
}

@inproceedings{kobayashi-2018-contextual,
    title = "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations",
    author = "Kobayashi, Sosuke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2072",
    doi = "10.18653/v1/N18-2072",
    pages = "452--457",
    abstract = "We propose a novel data augmentation for labeled sentences called contextual augmentation. We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions. Words predicted according to a context are numerous but appropriate for the augmentation of the original words. Furthermore, we retrofit a language model with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves classifiers based on the convolutional or recurrent neural networks.",
}

@article{WordNet,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = nov,
pages = {39–41},
numpages = {3}
}

@inproceedings{xu-etal-2020-data,
    title = "Data Augmentation for Multiclass Utterance Classification {--} A Systematic Study",
    author = "Xu, Binxia  and
      Qiu, Siyuan  and
      Zhang, Jie  and
      Wang, Yafang  and
      Shen, Xiaoyu  and
      de Melo, Gerard",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.479",
    doi = "10.18653/v1/2020.coling-main.479",
    pages = "5494--5506",
    abstract = "Utterance classification is a key component in many conversational systems. However, classifying real-world user utterances is challenging, as people may express their ideas and thoughts in manifold ways, and the amount of training data for some categories may be fairly limited, resulting in imbalanced data distributions. To alleviate these issues, we conduct a comprehensive survey regarding data augmentation approaches for text classification, including simple random resampling, word-level transformations, and neural text generation to cope with imbalanced data. Our experiments focus on multi-class datasets with a large number of data samples, which has not been systematically studied in previous work. The results show that the effectiveness of different data augmentation schemes depends on the nature of the dataset under consideration.",
}

@misc{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{aug2prev,
author = {Rizos, Georgios and Hemker, Konstantin and Schuller, Bj\"{o}rn},
title = {Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate-Speech Classification},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358040},
doi = {10.1145/3357384.3358040},
abstract = {In this paper, we address the issue of augmenting text data in supervised Natural Language Processing problems, exemplified by deep online hate speech classification. A great challenge in this domain is that although the presence of hate speech can be deleterious to the quality of service provided by social platforms, it still comprises only a tiny fraction of the content that can be found online, which can lead to performance deterioration due to majority class overfitting. To this end, we perform a thorough study on the application of deep learning to the hate speech detection problem: a) we propose three text-based data augmentation techniques aimed at reducing the degree of class imbalance and to maximise the amount of information we can extract from our limited resources and b) we apply them on a selection of top-performing deep architectures and hate speech databases in order to showcase their generalisation properties. The data augmentation techniques are based on a) synonym replacement based on word embedding vector closeness, b) warping of the word tokens along the padded sequence or c) class-conditional, recurrent neural language generation. Our proposed framework yields a significant increase in multi-class hate speech detection, outperforming the baseline in the largest online hate speech database by an absolute 5.7 increase in Macro-F1 score and 30% in hate speech class recall.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {991–1000},
numpages = {10},
keywords = {online hate speech detection, class imbalance, short text data augmentation},
location = {Beijing, China},
series = {CIKM '19}
}

@INPROCEEDINGS{7395756,
    author={F. Benites and E. Sapozhnikova},
    booktitle={2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
    title={HARAM: A Hierarchical ARAM Neural Network for Large-Scale Text Classification},
    year={2015},
    volume={},
    number={},
    pages={847-854},
    doi={10.1109/ICDMW.2015.14},
    ISSN={2375-9259},
    month={Nov},
}
@article{zhang2007ml,
  title={ML-KNN: A lazy learning approach to multi-label learning},
  author={Zhang, Min-Ling and Zhou, Zhi-Hua},
  journal={Pattern recognition},
  volume={40},
  number={7},
  pages={2038--2048},
  year={2007},
  publisher={Elsevier}
}
@misc{mollas2020ethos,
      title={ETHOS: an Online Hate Speech Detection Dataset}, 
      author={Ioannis Mollas and Zoe Chrysopoulou and Stamatis Karlos and Grigorios Tsoumakas},
      year={2020},
      eprint={2006.08328},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Poletto2020ResourcesAB,
  title={Resources and benchmark corpora for hate speech detection: a systematic review},
  author={Fabio Poletto and Valerio Basile and M. Sanguinetti and Cristina Bosco and V. Patti},
  booktitle={LREC 2020},
  year={2020}
}

@inproceedings{bowman-etal-2016-generating,
    title = "Generating Sentences from a Continuous Space",
    author = "Bowman, Samuel R.  and
      Vilnis, Luke  and
      Vinyals, Oriol  and
      Dai, Andrew  and
      Jozefowicz, Rafal  and
      Bengio, Samy",
    booktitle = "Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K16-1002",
    doi = "10.18653/v1/K16-1002",
    pages = "10--21",
}

@article{hasoc2019,
  title={Overview of the HASOC track at FIRE 2019: Hate Speech and Offensive Content Identification in Indo-European Languages},
  author={T. Mandl and Sandip Modha and P. Majumder and Daksh Patel and Mohana Dave and Chintak Mandalia and Aditya Patel},
  journal={Proceedings of the 11th Forum for Information Retrieval Evaluation},
  year={2019}
}

@misc{T5,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wordnet_,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@Inbook{Bisht2020,
author="Bisht, Akanksha
and Singh, Annapurna
and Bhadauria, H. S.
and Virmani, Jitendra
and Kriti",
editor="Jain, Shruti
and Paul, Sudip",
title="Detection of Hate Speech and Offensive Language in Twitter Data Using LSTM Model",
bookTitle="Recent Trends in Image and Signal Processing in Computer Vision",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="243--264",
isbn="978-981-15-2740-1",
doi="10.1007/978-981-15-2740-1_17",
url="https://doi.org/10.1007/978-981-15-2740-1_17"
}

@article{arora-etal-2018-linear,
    title = "Linear Algebraic Structure of Word Senses, with Applications to Polysemy",
    author = "Arora, Sanjeev  and
      Li, Yuanzhi  and
      Liang, Yingyu  and
      Ma, Tengyu  and
      Risteski, Andrej",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://www.aclweb.org/anthology/Q18-1034",
    doi = "10.1162/tacl_a_00034",
    pages = "483--495",
    abstract = "Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 {``}discourse atoms{''} that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.",
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{yap-etal-2020-adapting,
    title = "Adapting {BERT} for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences",
    author = "Yap, Boon Peng  and
      Koh, Andrew  and
      Chng, Eng Siong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.4",
    pages = "41--46"
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@inproceedings{feng-etal-2019-keep,
    title = "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange",
    author = "Feng, Steven Y.  and
      Li, Aaron W.  and
      Hoey, Jesse",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1272",
    doi = "10.18653/v1/D19-1272",
    pages = "2701--2711",
    abstract = "In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipeline{'}s success by its Semantic Text Exchange Score (STES): the ability to preserve the original text{'}s sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.",
}
@inbook{rake,
author = {Rose, Stuart and Engel, Dave and Cramer, Nick and Cowley, Wendy},
year = {2010},
month = {03},
pages = {1 - 20},
title = {Automatic Keyword Extraction from Individual Documents},
isbn = {9780470689646},
journal = {Text Mining: Applications and Theory},
doi = {10.1002/9780470689646.ch1}
}
@misc{feng2020genaug,
      title={GenAug: Data Augmentation for Finetuning Text Generators}, 
      author={Steven Y. Feng and Varun Gangal and Dongyeop Kang and Teruko Mitamura and Eduard Hovy},
      year={2020},
      eprint={2010.01794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{paws,
  title = {{PAWS: Paraphrase Adversaries from Word Scrambling}},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  booktitle = {Proc. of NAACL},
  year = {2019}
}

@misc{pegasus,
      title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
      author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
      year={2020},
      eprint={1912.08777},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lample2018unsupervised,
      title={Unsupervised Machine Translation Using Monolingual Corpora Only}, 
      author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
      year={2018},
      eprint={1711.00043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{summaformers,
   title={Summaformers @ LaySumm 20, LongSumm 20},
   url={http://dx.doi.org/10.18653/v1/2020.sdp-1.39},
   DOI={10.18653/v1/2020.sdp-1.39},
   journal={Proceedings of the First Workshop on Scholarly Document Processing},
   publisher={Association for Computational Linguistics},
   author={Ghosh Roy, Sayar and Pinnaparaju, Nikhil and Jain, Risubh and Gupta, Manish and Varma, Vasudeva},
   year={2020}
}

@inproceedings{gao-huang-2017-detecting,
    title = "Detecting Online Hate Speech Using Context Aware Models",
    author = "Gao, Lei  and
      Huang, Ruihong",
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://doi.org/10.26615/978-954-452-049-6_036",
    doi = "10.26615/978-954-452-049-6_036",
    pages = "260--266",
    abstract = "In the wake of a polarizing election, the cyber world is laden with hate speech. Context accompanying a hate speech text is useful for identifying hate speech, which however has been largely overlooked in existing datasets and hate speech detection models. In this paper, we provide an annotated corpus of hate speech with context information well kept. Then we propose two types of hate speech detection models that incorporate context information, a logistic regression model with context features and a neural network model with learning components for context. Our evaluation shows that both models outperform a strong baseline by around 3{\%} to 4{\%} in F1 score and combining these two models further improve the performance by another 7{\%} in F1 score.",
}
