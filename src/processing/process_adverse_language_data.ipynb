{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process_adverse_language_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDjn/gMbbNhTmBzvHkBCi9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Augment4Gains/blob/main/process_adverse_language_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX0LVUBPhKJT"
      },
      "source": [
        "# Processing dataset from 'Detecting Online Hate Speech Using Context Aware Models'\n",
        "# Raw data can be found at:\n",
        "# github.com/t-davidson/hate-speech-and-offensive-language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-gxFU-7heXX"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBy1Mr2-hyM3"
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QgchmYWiBQv"
      },
      "source": [
        "file_name = 'labeled_data.csv'\n",
        "see_index = True\n",
        "\n",
        "data = []\n",
        "\n",
        "file = open(file_name, 'r')\n",
        "\n",
        "file_reader = csv.reader(file, delimiter = \",\")\n",
        "for line in file_reader:\n",
        "  # line[6] contains the Tweet text\n",
        "  # line[5] contains the Tweet label\n",
        "  if see_index == True:\n",
        "    see_index = False\n",
        "    continue\n",
        "  unit = {}\n",
        "  base = str(line[6])\n",
        "  cleaned_source = tweet_proc.clean(base.replace('\\n', ' '))\n",
        "  unit['source'] = cleaned_source\n",
        "  \n",
        "  if str(line[5]).strip() == '2':\n",
        "    unit['target'] = 0\n",
        "  else:\n",
        "    unit['target'] = 1\n",
        "  data.append(unit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU40l8OKicxs",
        "outputId": "e89e9f01-332a-4d58-c061-c3b84b124f16"
      },
      "source": [
        "indices = [id for id in range(len(data))]\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_size = int((7 / 10) * len(data))\n",
        "val_size = int((1 / 10) * len(data))\n",
        "test_size = len(data) - (train_size + val_size)\n",
        "\n",
        "print('Train Set Size: ' + str(train_size))\n",
        "print('Validation Set Size: ' + str(val_size))\n",
        "print('Test Set Size: ' + str(test_size))\n",
        "\n",
        "train_indices = indices[0: train_size]\n",
        "val_indices = indices[train_size: train_size + val_size]\n",
        "test_indices = indices[train_size + val_size: ]\n",
        "\n",
        "# Defining the splits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Set Size: 17348\n",
            "Validation Set Size: 2478\n",
            "Test Set Size: 4957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA9HgPzBj7GL"
      },
      "source": [
        "train, val, test = [], [], []\n",
        "\n",
        "for index in indices:\n",
        "  if index in train_indices:\n",
        "    train.append(data[index])\n",
        "  elif index in val_indices:\n",
        "    val.append(data[index])\n",
        "  else:\n",
        "    test.append(data[index])\n",
        "\n",
        "# Splits created"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4xpbRnTmHrq"
      },
      "source": [
        "# Saving the splits\n",
        "with open('train.json', 'w+') as f:\n",
        "  json.dump(train, f)\n",
        "\n",
        "with open('val.json', 'w+') as f:\n",
        "  json.dump(val, f)\n",
        "\n",
        "with open('test.json', 'w+') as f:\n",
        "  json.dump(test, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmZQj4E6n9n_"
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}