{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pegasus_paraphrase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKDejEfrrkZh"
      },
      "source": [
        "# Using a pre-trained Transformer Encoder-Decoder based Paraphraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oJZEy3KtphC"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Getting necessary libraries\n",
        "!pip install -U transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "import json\n",
        "import torch\n",
        "import nltk\n",
        "import sentencepiece\n",
        "from tqdm import tqdm\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7IF3tqit3Uv",
        "outputId": "f9f0bc32-3d8f-4020-a459-1293454cf1ae"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EpUtRKc35Bf"
      },
      "source": [
        "# Using a standard model\n",
        "tokenizer_model_name = 'google/pegasus-large'\n",
        "\n",
        "# Using a pre-trained community model\n",
        "paraphrasing_model_name = 'tuner007/pegasus_paraphrase'\n",
        "\n",
        "torch_device = 'cuda'\n",
        "if torch.cuda.is_available() == False:\n",
        "  torch_device = 'cpu'\n",
        "\n",
        "tokenizer = PegasusTokenizer.from_pretrained(tokenizer_model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(paraphrasing_model_name).to(torch_device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2FxfvLyK63A"
      },
      "source": [
        "global_return_sequences = 5\n",
        "\n",
        "def get_unit_paraphrase(input_text, num_return_sequences = global_return_sequences, num_beams = 10):\n",
        "  max_len = 60\n",
        "  batch = tokenizer([input_text], \n",
        "                    truncation = True,\n",
        "                    padding = 'longest',\n",
        "                    max_length = max_len,\n",
        "                    return_tensors = 'pt').to(torch_device)\n",
        "\n",
        "  translated = model.generate(**batch,\n",
        "                              max_length = max_len,\n",
        "                              num_beams = num_beams,\n",
        "                              num_return_sequences = num_return_sequences,\n",
        "                              temperature = 1.5)\n",
        "  \n",
        "  targets = tokenizer.batch_decode(translated,\n",
        "                                  skip_special_tokens = True)\n",
        "  \n",
        "  return targets\n",
        "\n",
        "def get_paraphrase(input_text, num_return_sequences = global_return_sequences, num_beams = 10):\n",
        "  preprocess_len = 52\n",
        "  complete_paraphrases = ['', '', '', '', '']\n",
        "\n",
        "  sentences = nltk.sent_tokenize(input_text)\n",
        "  for sentence in sentences:\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    count = len(tokens)\n",
        "    if count > preprocess_len:\n",
        "      continue\n",
        "    try:\n",
        "      sentence_paraphrases = get_unit_paraphrase(sentence)\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    for index, unit in enumerate(sentence_paraphrases):\n",
        "      complete_paraphrases[index] += unit + ' '\n",
        "\n",
        "  return complete_paraphrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt8CTX4avo4h",
        "outputId": "61b07457-7dc5-453f-b433-2f89230e4ea5"
      },
      "source": [
        "# Viewing Sample Paraphrases\n",
        "\n",
        "examples = ['you should watch louis le vau \\'s latest video . steven oh of tyt is disturbing as hell and makes me hope that jimmy dore wakes the left up .',\n",
        "            'kill yourself you whiny , self-righteous faggot .',\n",
        "            'but why do they make that face']\n",
        "\n",
        "for example in examples:\n",
        "  print('Source: ' + str(example))\n",
        "  response = get_paraphrase(example)\n",
        "  print('Primary Paraphrase: ' + str(response[0]))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: you should watch louis le vau 's latest video . steven oh of tyt is disturbing as hell and makes me hope that jimmy dore wakes the left up .\n",
            "Primary Paraphrase: louis le vau has a new video. I hope that jimmy dore wakes the left up because steven oh of tyt is disturbing. \n",
            "\n",
            "Source: kill yourself you whiny , self-righteous faggot .\n",
            "Primary Paraphrase: You are self-righteous and should kill yourself. \n",
            "\n",
            "Source: but why do they make that face\n",
            "Primary Paraphrase: Why do they make that face? \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cx3G5pYvu31"
      },
      "source": [
        "# Generating the Augmented Training Data\n",
        "set_type = 'twitter'\n",
        "\n",
        "# Reference to the absolute path in Google Drive\n",
        "data_path = 'drive/My Drive/Augment4Gains/data/' + set_type\n",
        "\n",
        "with open(data_path + '/' + 'train.json', 'r+') as f:\n",
        "  raw_train = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj_zq6f0FvLA",
        "outputId": "87f6ba29-f623-4861-cb59-ae2117d32a23"
      },
      "source": [
        "# Getting the Augmented Datapoints\n",
        "augmented_data = []\n",
        "overwrite_data = True\n",
        "save_name = data_path + '/' + 'sent_wise_paraphrased_train.json'\n",
        "limit = len(raw_train)\n",
        "minimum_length = 4\n",
        "interval = 500\n",
        "\n",
        "test_mode = True\n",
        "if test_mode == True:\n",
        "  interval = 2\n",
        "  limit = 50\n",
        "\n",
        "backup_present = path.isfile(save_name)\n",
        "done = 0\n",
        "\n",
        "if backup_present:\n",
        "  print('Pre-processed Data Backup Found: ' + str(backup_present), flush = True)\n",
        "  with open(save_name, 'r+') as f:\n",
        "    augmented_data = json.load(f)\n",
        "  done = int(len(augmented_data) / global_return_sequences)\n",
        "  print('Starting from ' + str(done) + ' onwards.', flush = True)\n",
        "\n",
        "for index in tqdm(range(done, limit)):\n",
        "  unit = raw_train[index]\n",
        "  if index > limit - 1:\n",
        "    break\n",
        "\n",
        "  try:\n",
        "    raw_text = str(unit['source'].replace('\\n', ' '))\n",
        "    targets = get_paraphrase(raw_text)\n",
        "  except:\n",
        "    pass\n",
        "    continue\n",
        "  \n",
        "  for target in targets:\n",
        "    token_count = len(nltk.word_tokenize(target))\n",
        "    if token_count < minimum_length:\n",
        "      continue\n",
        "    new_unit = unit.copy()\n",
        "    \n",
        "    if 'type' in new_unit:\n",
        "      new_unit.pop('type')\n",
        "    if 'set' in new_unit:\n",
        "      new_unit.pop('set')\n",
        "\n",
        "    new_unit['source'] = target\n",
        "    augmented_data.append(new_unit)\n",
        "\n",
        "  if index % interval == 0 and overwrite_data == True:\n",
        "      with open(save_name, 'w+') as f:\n",
        "        json.dump(augmented_data, f)\n",
        "\n",
        "if overwrite_data == True:\n",
        "  with open(save_name, 'w+') as f:\n",
        "    json.dump(augmented_data, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre-processed Data Backup Found: True\n",
            "Starting from 24 onwards.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26/26 [00:13<00:00,  1.89it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkswWKTF040"
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}