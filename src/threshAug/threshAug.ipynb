{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "threshAug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLiXePhnJDba",
        "outputId": "0eb3e6f9-7e46-41e8-85ca-f870f1025dc8"
      },
      "source": [
        "!apt install libopenblas-base libomp-dev\n",
        "!pip install faiss\n",
        "import numpy as np \n",
        "import faiss \n",
        "import json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-base is already the newest version (0.2.20+ds-4).\n",
            "libopenblas-base set to manually installed.\n",
            "Suggested packages:\n",
            "  libomp-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-dev libomp5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 804 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n",
            "Fetched 239 kB in 2s (130 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Selecting previously unselected package libomp-dev.\n",
            "Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp-dev (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp-dev (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqk_v5NXcX7a",
        "outputId": "de9fe35b-8bc2-4c2e-87f8-c1d1c409959c"
      },
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "drive.mount('/content/drive')\n",
        "model = api.load('glove-twitter-50')\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWSJpsyKqHjH",
        "outputId": "1717c598-fb69-45d2-8ed5-4f68ffaa2720"
      },
      "source": [
        "threshold = 0.75\n",
        "N = 1\n",
        "with open('/content/drive/My Drive/Augment4Gains/data/reddit/train.json') as f:\n",
        "  data = json.load(f)\n",
        "  for each_data in data:\n",
        "    sentence = each_data['source']\n",
        "    print(sentence)\n",
        "    #processed_sentence = re.sub('[^a-zA-Z]', ' ', processed_sentence )\n",
        "    processed_sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    words = word_tokenize(processed_sentence)\n",
        "    #pos_tagging goes here\n",
        "    word_pos = nltk.pos_tag(words)\n",
        "    for idx, word in enumerate(words):\n",
        "      if word in model.vocab:\n",
        "        if word_pos[idx][1] in ['VB','NN','JJ']:\n",
        "          #get words with highest cosine similarity\n",
        "          replacements = model.most_similar(positive=word, topn=N)\n",
        "          #keep only words that pass the threshold\n",
        "          replacements = [replacements[i][0] for i in range(N) if replacements[i][1] > threshold]\n",
        "          #check for POS tag equality, dismiss if unequal\n",
        "          replacements = [elem for elem in replacements if nltk.pos_tag([elem.lower()])[0][1] == word_pos[idx][1]]\n",
        "          if replacements:\n",
        "            words[idx] = replacements[0]\n",
        "    new_sentence = TreebankWordDetokenizer().detokenize(words)\n",
        "    print(new_sentence)\n",
        "    print('')\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if you give me that salsa i will love you forever . spicy is the shit .\n",
            "if you give me that merengue i will love you forever . spicy is the damn.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}