{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "threshAug.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqk_v5NXcX7a",
        "outputId": "8d28105a-652f-4f76-bb6c-27c08096b657"
      },
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import gensim.downloader as api\n",
        "import keras\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZcAVtjUjMYm",
        "outputId": "f7be9ecd-04e4-487e-8793-6bb42b721ee6"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
        "import numpy as np\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Parameters: change to experiment different configurations\n",
        "SEQUENCE_LEN = 10\n",
        "MIN_WORD_FREQUENCY = 10\n",
        "STEP = 1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "    for i in np.random.permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "# Data generator for fit and evaluate\n",
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
        "        y = np.zeros((batch_size), dtype=np.int32)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
        "                x[i, t] = word_indices[w]\n",
        "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
        "            index = index + 1\n",
        "        yield x, y\n",
        "\n",
        "\n",
        "def get_model(dropout=0.2):\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(words), output_dim=100))\n",
        "    model.add(Bidirectional(LSTM(128)))\n",
        "    if dropout > 0:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(len(words)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
        "\n",
        "    # Randomly pick a seed sequence\n",
        "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
        "    seed = (sentences+sentences_test)[seed_index]\n",
        "\n",
        "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "        sentence = seed\n",
        "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
        "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "        examples_file.write(' '.join(sentence))\n",
        "\n",
        "        for i in range(50):\n",
        "            x_pred = np.zeros((1, SEQUENCE_LEN))\n",
        "            for t, word in enumerate(sentence):\n",
        "                x_pred[0, t] = word_indices[word]\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_word = indices_word[next_index]\n",
        "\n",
        "            sentence = sentence[1:]\n",
        "            sentence.append(next_word)\n",
        "\n",
        "            examples_file.write(\" \"+next_word)\n",
        "        examples_file.write('\\n')\n",
        "    examples_file.write('='*80 + '\\n')\n",
        "    examples_file.flush()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Argument check\n",
        "    examples = \"./examples.txt\"\n",
        "\n",
        "    if not os.path.isdir('./checkpoints/'):\n",
        "        os.makedirs('./checkpoints/')\n",
        "\n",
        "    # with io.open(corpus, encoding='utf-8') as f:\n",
        "    #     text = f.read().lower().replace('\\n', ' \\n ')\n",
        "    # print('Corpus length in characters:', len(text))\n",
        "\n",
        "    # text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "    # print('Corpus length in words:', len(text_in_words))\n",
        "    text_in_words = []\n",
        "    with open('/content/drive/My Drive/Augment4Gains/data/reddit/train.json') as f:\n",
        "      data = json.load(f)\n",
        "      for each_data in data:\n",
        "        sentence = each_data['source']\n",
        "        # sentences.append(sentence)\n",
        "        processed_sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "        words = word_tokenize(processed_sentence)\n",
        "        text_in_words.extend(words)\n",
        "    \n",
        "    print('Corpus length in words:', len(text_in_words))\n",
        "    # Calculate word frequency\n",
        "    word_freq = {}\n",
        "    for word in text_in_words:\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "    ignored_words = set()\n",
        "    for k, v in word_freq.items():\n",
        "        if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "            ignored_words.add(k)\n",
        "\n",
        "    words = set(text_in_words)\n",
        "    print('Unique words before ignoring:', len(words))\n",
        "    print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
        "    words = sorted(set(words) - ignored_words)\n",
        "    print('Unique words after ignoring:', len(words))\n",
        "\n",
        "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "    # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
        "    sentences = []\n",
        "    next_words = []\n",
        "    ignored = 0\n",
        "    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "        # Only add the sequences where no word is in ignored_words\n",
        "        if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "            next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "        else:\n",
        "            ignored = ignored + 1\n",
        "    print('Ignored sequences:', ignored)\n",
        "    print('Remaining sequences:', len(sentences))\n",
        "\n",
        "    # x, y, x_test, y_test\n",
        "    (sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(\n",
        "        sentences, next_words\n",
        "    )\n",
        "\n",
        "    model = get_model()\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    file_path = \"./checkpoints/GENAUG-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
        "                \"loss{loss:.4f}-accuracy{accuracy:.4f}-val_loss{val_loss:.4f}-val_accuracy{val_accuracy:.4f}\" % \\\n",
        "                (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
        "    print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
        "    callbacks_list = [checkpoint, print_callback, early_stopping]\n",
        "\n",
        "    examples_file = open(examples, \"w\")\n",
        "    model.fit(generator(sentences, next_words, BATCH_SIZE),\n",
        "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
        "                        epochs=100,\n",
        "                        callbacks=callbacks_list,\n",
        "                        validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
        "                        validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length in words: 794259\n",
            "Unique words before ignoring: 31458\n",
            "Ignoring words with frequency < 10\n",
            "Unique words after ignoring: 4773\n",
            "Ignored sequences: 413886\n",
            "Remaining sequences: 380363\n",
            "Shuffling sentences\n",
            "Size of training set = 372755\n",
            "Size of test set = 7608\n",
            "Build model...\n",
            "Epoch 1/100\n",
            "11649/11649 [==============================] - 124s 10ms/step - loss: 5.8625 - accuracy: 0.0934 - val_loss: 5.0585 - val_accuracy: 0.1633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./checkpoints/GENAUG-epoch001-words4773-sequence10-minfreq10-loss5.4988-accuracy0.1272-val_loss5.0585-val_accuracy0.1633/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./checkpoints/GENAUG-epoch001-words4773-sequence10-minfreq10-loss5.4988-accuracy0.1272-val_loss5.0585-val_accuracy0.1633/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "11649/11649 [==============================] - 122s 10ms/step - loss: 5.0010 - accuracy: 0.1694 - val_loss: 4.8942 - val_accuracy: 0.1787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./checkpoints/GENAUG-epoch002-words4773-sequence10-minfreq10-loss4.9333-accuracy0.1748-val_loss4.8942-val_accuracy0.1787/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./checkpoints/GENAUG-epoch002-words4773-sequence10-minfreq10-loss4.9333-accuracy0.1748-val_loss4.8942-val_accuracy0.1787/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/100\n",
            "11649/11649 [==============================] - 123s 11ms/step - loss: 4.7487 - accuracy: 0.1857 - val_loss: 4.8410 - val_accuracy: 0.1843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./checkpoints/GENAUG-epoch003-words4773-sequence10-minfreq10-loss4.7074-accuracy0.1887-val_loss4.8410-val_accuracy0.1843/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./checkpoints/GENAUG-epoch003-words4773-sequence10-minfreq10-loss4.7074-accuracy0.1887-val_loss4.8410-val_accuracy0.1843/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/100\n",
            "11649/11649 [==============================] - 125s 11ms/step - loss: 4.5672 - accuracy: 0.1972 - val_loss: 4.8493 - val_accuracy: 0.1879\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}